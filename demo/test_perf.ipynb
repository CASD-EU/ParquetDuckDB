{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-27T15:51:21.233497Z",
     "start_time": "2025-01-27T15:51:20.752077Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T15:51:26.795277Z",
     "start_time": "2025-01-27T15:51:21.373699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark=SparkSession.builder.master(\"local[4]\") \\\n",
    "                  .appName(\"ReadWriteParquet\") \\\n",
    "                  .config(\"spark.sql.legacy.parquet.nanosAsLong\", \"true\") \\\n",
    "                  .getOrCreate()\n"
   ],
   "id": "ced482640a1fddc0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/27 16:51:23 WARN Utils: Your hostname, pliu-ubuntu24 resolves to a loopback address: 127.0.1.1; using 192.168.30.128 instead (on interface ens33)\n",
      "25/01/27 16:51:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/27 16:51:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/27 16:51:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# configuration",
   "id": "17e0ee13031602dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T15:51:30.834253Z",
     "start_time": "2025-01-27T15:51:30.827949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_path = Path.cwd().parent / \"data\"\n",
    "\n",
    "fr_immo_raw_path = (data_path / \"fr_immo_transactions.parquet\").as_posix()\n",
    "fr_immo_valid_path = (data_path / \"fr_immo_transactions_valid_ts.parquet\").as_posix()\n",
    "fr_immo_csv_path = (data_path / \"fr_immo_transactions.csv\").as_posix()\n",
    "\n",
    "date_col_name = \"date_transaction\"\n"
   ],
   "id": "22a348b81de98149",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Performance test csv vs parquet\n",
    "\n",
    "In this part, we will test the query performance between csv and parquet.\n",
    "\n",
    "1. row_counts\n",
    "2. group_by\n",
    "3. filter"
   ],
   "id": "6fbdfdea55a8f6d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T16:07:58.384270Z",
     "start_time": "2025-01-27T16:07:57.830643Z"
    }
   },
   "cell_type": "code",
   "source": "fr_immo_parquet_df = spark.read.parquet(fr_immo_valid_path)",
   "id": "66637272e90694c5",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T16:08:00.793837Z",
     "start_time": "2025-01-27T16:07:59.883583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "fr_immo_parquet_df.count()"
   ],
   "id": "3716c9e802314b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.42 ms, sys: 3.65 ms, total: 10.1 ms\n",
      "Wall time: 879 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9141573"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T16:08:02.872427Z",
     "start_time": "2025-01-27T16:08:02.864356Z"
    }
   },
   "cell_type": "code",
   "source": "fr_immo_parquet_df.printSchema()",
   "id": "e3a5a53e626f1f60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_transaction: integer (nullable = true)\n",
      " |-- date_transaction: timestamp (nullable = true)\n",
      " |-- prix: double (nullable = true)\n",
      " |-- departement: string (nullable = true)\n",
      " |-- id_ville: integer (nullable = true)\n",
      " |-- ville: string (nullable = true)\n",
      " |-- code_postal: integer (nullable = true)\n",
      " |-- adresse: string (nullable = true)\n",
      " |-- type_batiment: string (nullable = true)\n",
      " |-- n_pieces: integer (nullable = true)\n",
      " |-- surface_habitable: integer (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T16:09:12.190467Z",
     "start_time": "2025-01-27T16:08:41.916852Z"
    }
   },
   "cell_type": "code",
   "source": "fr_immo_csv_df = spark.read.option(\"header\",True).option(\"inferSchema\", True).csv(fr_immo_csv_path)\n",
   "id": "4129f6eee302c3b9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T16:09:19.149326Z",
     "start_time": "2025-01-27T16:09:19.135974Z"
    }
   },
   "cell_type": "code",
   "source": "fr_immo_csv_df.printSchema()",
   "id": "9409b1321d46faa8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_transaction: integer (nullable = true)\n",
      " |-- date_transaction: timestamp (nullable = true)\n",
      " |-- prix: double (nullable = true)\n",
      " |-- departement: integer (nullable = true)\n",
      " |-- id_ville: integer (nullable = true)\n",
      " |-- ville: string (nullable = true)\n",
      " |-- code_postal: integer (nullable = true)\n",
      " |-- adresse: string (nullable = true)\n",
      " |-- type_batiment: string (nullable = true)\n",
      " |-- n_pieces: integer (nullable = true)\n",
      " |-- surface_habitable: integer (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T16:09:31.019185Z",
     "start_time": "2025-01-27T16:09:29.645569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "fr_immo_csv_df.count()"
   ],
   "id": "9daa2bc0dc97b71e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:==================================>                      (6 + 4) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.33 ms, sys: 2.47 ms, total: 8.8 ms\n",
      "Wall time: 1.36 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9141573"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T16:09:57.197248Z",
     "start_time": "2025-01-27T16:09:57.190693Z"
    }
   },
   "cell_type": "code",
   "source": "batiment_typ_col_name = \"type_batiment\"",
   "id": "2608af003801764e",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T16:10:00.849637Z",
     "start_time": "2025-01-27T16:10:00.166126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "fr_immo_parquet_df.groupby(batiment_typ_col_name).count().show()"
   ],
   "id": "b0ce5108baeceb71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|type_batiment|  count|\n",
      "+-------------+-------+\n",
      "|  Appartement|4079137|\n",
      "|       Maison|5062436|\n",
      "+-------------+-------+\n",
      "\n",
      "CPU times: user 3.88 ms, sys: 551 Î¼s, total: 4.43 ms\n",
      "Wall time: 679 ms\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T16:10:12.658824Z",
     "start_time": "2025-01-27T16:10:06.477530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "fr_immo_csv_df.groupby(batiment_typ_col_name).count().show()"
   ],
   "id": "5d74c966018b76af",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|type_batiment|  count|\n",
      "+-------------+-------+\n",
      "|  Appartement|4079137|\n",
      "|       Maison|5062436|\n",
      "+-------------+-------+\n",
      "\n",
      "CPU times: user 16.6 ms, sys: 6.98 ms, total: 23.5 ms\n",
      "Wall time: 6.17 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T16:15:17.075359Z",
     "start_time": "2025-01-27T16:15:16.764817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "code_postal_montrouge = 92120\n",
    "\n",
    "total_transaction_montrouge = fr_immo_parquet_df.filter(col(\"code_postal\") == code_postal_montrouge).count()\n",
    "print(f\"Total transaction montrouge: {total_transaction_montrouge}\")"
   ],
   "id": "ea9c951602b5c290",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transaction montrouge: 7361\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7241ca4bbfe8c94e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T16:15:49.051840Z",
     "start_time": "2025-01-27T16:15:44.328881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_transaction_montrouge = fr_immo_csv_df.filter(col(\"code_postal\")== code_postal_montrouge).count()\n",
    "print(f\"Total transaction montrouge: {total_transaction_montrouge}\")"
   ],
   "id": "d996aea57d1a0cfa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transaction montrouge: 7361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Compatibility problems\n",
    "\n",
    "As there are many libraries that can write parquet files, there are some compatibility problems.\n",
    "\n",
    "### Timestamp data type\n",
    "\n",
    "The default Timestamp data type implementation in **pyarrow/pandas is INT64 (TIMESTAMP(NANOS))**.\n",
    "\n",
    "The default Timestamp data type implementation in **spark is INT64 (TIMESTAMP(MICROS)) or INT96 (NANOS)**.\n",
    "\n",
    "INT96 (NANOS) is deprecated in the newer spark version. You can still activate in Spark environments with `config(\"spark.sql.legacy.parquet.nanosAsLong\", \"true\")`\n",
    "\n",
    "In the below example, we use pyspark to read a parquet file which is generated by using the pandas/pyarrow. You may receive an error message (based on your spark version)\n",
    "\n",
    "```java\n",
    "org.apache.spark.sql.AnalysisException: Illegal Parquet type: INT64 (TIMESTAMP(NANOS,false)).\n",
    "```"
   ],
   "id": "7efb4e02e2afb6af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T15:24:40.210827Z",
     "start_time": "2025-01-27T15:24:37.060126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fr_immo_raw_df = spark.read.parquet(fr_immo_raw_path)\n",
    "print(fr_immo_raw_df.count())\n"
   ],
   "id": "51c85c5f4ebe74ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9141573\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T15:28:43.787714Z",
     "start_time": "2025-01-27T15:28:43.513501Z"
    }
   },
   "cell_type": "code",
   "source": "fr_immo_raw_df.select([date_col_name]).show(5)",
   "id": "6db13988e3c31f15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|   date_transaction|\n",
      "+-------------------+\n",
      "|1388620800000000000|\n",
      "|1388620800000000000|\n",
      "|1388620800000000000|\n",
      "|1388620800000000000|\n",
      "|1388707200000000000|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T15:26:04.884737Z",
     "start_time": "2025-01-27T15:26:04.866374Z"
    }
   },
   "cell_type": "code",
   "source": "fr_immo_raw_df.printSchema()",
   "id": "2dacd0b7ea2600b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_transaction: integer (nullable = true)\n",
      " |-- date_transaction: long (nullable = true)\n",
      " |-- prix: double (nullable = true)\n",
      " |-- departement: string (nullable = true)\n",
      " |-- id_ville: integer (nullable = true)\n",
      " |-- ville: string (nullable = true)\n",
      " |-- code_postal: integer (nullable = true)\n",
      " |-- adresse: string (nullable = true)\n",
      " |-- type_batiment: string (nullable = true)\n",
      " |-- n_pieces: integer (nullable = true)\n",
      " |-- surface_habitable: integer (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T15:28:51.899625Z",
     "start_time": "2025-01-27T15:28:51.276463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fr_immo_valid_df = spark.read.parquet(fr_immo_valid_path)\n",
    "fr_immo_valid_df.select([date_col_name]).show(5)"
   ],
   "id": "974977a2c3a78998",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|    date_transaction|\n",
      "+--------------------+\n",
      "|2013-12-31 10:25:...|\n",
      "|2013-12-31 10:25:...|\n",
      "|2013-12-31 10:25:...|\n",
      "|2013-12-31 10:25:...|\n",
      "|2014-01-01 10:25:...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "705511fd3d51d470"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e3d5d95ae8d9b7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cd8503e798f5d51d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
